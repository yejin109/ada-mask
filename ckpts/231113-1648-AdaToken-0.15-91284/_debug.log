[2023-11-13 16:48:14,221, INFO] : Cache at /data1/ay0119/hf-cache
[2023-11-13 16:48:14,221, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231113-1648-AdaToken-0.15-91284
[2023-11-13 16:48:14,221, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:48:14,228, INFO] : Cache at /data1/ay0119/hf-cache
[2023-11-13 16:48:14,228, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231113-1648-AdaToken-0.15-91284
[2023-11-13 16:48:14,228, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:48:28,730, INFO] : Cache at /data1/ay0119/hf-cache
[2023-11-13 16:48:28,730, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231113-1648-AdaToken-0.15-91284
[2023-11-13 16:48:28,730, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:48:28,859, INFO] : Cache at /data1/ay0119/hf-cache
[2023-11-13 16:48:28,859, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231113-1648-AdaToken-0.15-91284
[2023-11-13 16:48:28,859, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:49:40,006, INFO] : bookcorpus raw data 
Dataset({
    features: ['text'],
    num_rows: 74004228
})
[2023-11-13 16:49:40,007, INFO] : wikipedia raw data 
Dataset({
    features: ['text'],
    num_rows: 6458670
})
[2023-11-13 16:49:40,007, INFO] : concat raw data 
Dataset({
    features: ['text'],
    num_rows: 80462898
})
[2023-11-13 16:49:40,007, INFO] : MLM process function started
[2023-11-13 16:49:40,021, INFO] : bookcorpus raw data 
Dataset({
    features: ['text'],
    num_rows: 74004228
})
[2023-11-13 16:49:40,021, INFO] : wikipedia raw data 
Dataset({
    features: ['text'],
    num_rows: 6458670
})
[2023-11-13 16:49:40,021, INFO] : concat raw data 
Dataset({
    features: ['text'],
    num_rows: 80462898
})
[2023-11-13 16:49:40,021, INFO] : MLM process function started
[2023-11-13 16:49:41,443, INFO] : MLM process function Done
[2023-11-13 16:49:41,469, INFO] : MLM process function Done
[2023-11-13 16:49:41,482, INFO] : Split started
[2023-11-13 16:49:41,510, INFO] : Split started
[2023-11-13 16:49:42,292, INFO] : Split Done
[2023-11-13 16:49:42,293, DEBUG] : grouping took 1.4215972423553467
[2023-11-13 16:49:42,294, DEBUG] : split took 0.8088293075561523
[2023-11-13 16:49:42,326, INFO] : Split Done
[2023-11-13 16:49:42,327, DEBUG] : grouping took 1.4620745182037354
[2023-11-13 16:49:42,328, DEBUG] : split took 0.811854362487793
[2023-11-13 16:49:42,582, INFO] : Dataset 
	 DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 10743786
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 108
    })
})
[2023-11-13 16:49:42,583, DEBUG] : seed = 91284
[2023-11-13 16:49:42,584, DEBUG] : device = cuda:4
[2023-11-13 16:49:42,585, DEBUG] : cache_dir = /data1/ay0119/hf-cache
[2023-11-13 16:49:42,586, DEBUG] : model_type = google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:49:42,587, DEBUG] : ckpt = None
[2023-11-13 16:49:42,587, DEBUG] : data = all
[2023-11-13 16:49:42,587, DEBUG] : train_test_split = 1e-05
[2023-11-13 16:49:42,587, DEBUG] : max_seq_length = 512
[2023-11-13 16:49:42,587, DEBUG] : lr = 0.0001
[2023-11-13 16:49:42,587, DEBUG] : lr_scheduler = linear
[2023-11-13 16:49:42,587, DEBUG] : warmup_steps = 10000
[2023-11-13 16:49:42,587, DEBUG] : adam_beta1 = 0.9
[2023-11-13 16:49:42,587, DEBUG] : adam_beta2 = 0.999
[2023-11-13 16:49:42,587, DEBUG] : adam_eps = 1e-08
[2023-11-13 16:49:42,587, DEBUG] : wd = 0.01
[2023-11-13 16:49:42,587, DEBUG] : dropout = 0.1
[2023-11-13 16:49:42,587, DEBUG] : epochs = 40
[2023-11-13 16:49:42,587, DEBUG] : b_train = 16
[2023-11-13 16:49:42,587, DEBUG] : max_steps = 250000
[2023-11-13 16:49:42,587, DEBUG] : gradient_accumulation_steps = 4
[2023-11-13 16:49:42,587, DEBUG] : p = 0.15
[2023-11-13 16:49:42,587, DEBUG] : mask_tolerance = 0.01
[2023-11-13 16:49:42,587, DEBUG] : mask_increment = 0.0025
[2023-11-13 16:49:42,588, DEBUG] : ada_token = True
[2023-11-13 16:49:42,588, DEBUG] : ada_memo = False
[2023-11-13 16:49:42,588, DEBUG] : cosine = False
[2023-11-13 16:49:42,588, DEBUG] : step = False
[2023-11-13 16:49:42,588, DEBUG] : train = True
[2023-11-13 16:49:42,588, DEBUG] : test = False
[2023-11-13 16:49:42,588, DEBUG] : logging_steps = 25000
[2023-11-13 16:49:42,588, DEBUG] : save_steps = 50000
[2023-11-13 16:49:42,588, DEBUG] : local_rank = 0
[2023-11-13 16:49:42,588, DEBUG] : world_size = 0
[2023-11-13 16:49:42,731, INFO] : Dataset 
	 DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 10743786
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 108
    })
})
[2023-11-13 16:49:42,732, DEBUG] : seed = 91284
[2023-11-13 16:49:42,733, DEBUG] : device = cuda:4
[2023-11-13 16:49:42,734, DEBUG] : cache_dir = /data1/ay0119/hf-cache
[2023-11-13 16:49:42,735, DEBUG] : model_type = google/bert_uncased_L-2_H-128_A-2
[2023-11-13 16:49:42,736, DEBUG] : ckpt = None
[2023-11-13 16:49:42,736, DEBUG] : data = all
[2023-11-13 16:49:42,737, DEBUG] : train_test_split = 1e-05
[2023-11-13 16:49:42,737, DEBUG] : max_seq_length = 512
[2023-11-13 16:49:42,737, DEBUG] : lr = 0.0001
[2023-11-13 16:49:42,737, DEBUG] : lr_scheduler = linear
[2023-11-13 16:49:42,737, DEBUG] : warmup_steps = 10000
[2023-11-13 16:49:42,737, DEBUG] : adam_beta1 = 0.9
[2023-11-13 16:49:42,737, DEBUG] : adam_beta2 = 0.999
[2023-11-13 16:49:42,737, DEBUG] : adam_eps = 1e-08
[2023-11-13 16:49:42,737, DEBUG] : wd = 0.01
[2023-11-13 16:49:42,737, DEBUG] : dropout = 0.1
[2023-11-13 16:49:42,737, DEBUG] : epochs = 40
[2023-11-13 16:49:42,737, DEBUG] : b_train = 16
[2023-11-13 16:49:42,737, DEBUG] : max_steps = 250000
[2023-11-13 16:49:42,737, DEBUG] : gradient_accumulation_steps = 4
[2023-11-13 16:49:42,737, DEBUG] : p = 0.15
[2023-11-13 16:49:42,737, DEBUG] : mask_tolerance = 0.01
[2023-11-13 16:49:42,737, DEBUG] : mask_increment = 0.0025
[2023-11-13 16:49:42,737, DEBUG] : ada_token = True
[2023-11-13 16:49:42,737, DEBUG] : ada_memo = False
[2023-11-13 16:49:42,737, DEBUG] : cosine = False
[2023-11-13 16:49:42,737, DEBUG] : step = False
[2023-11-13 16:49:42,737, DEBUG] : train = True
[2023-11-13 16:49:42,737, DEBUG] : test = False
[2023-11-13 16:49:42,737, DEBUG] : logging_steps = 25000
[2023-11-13 16:49:42,737, DEBUG] : save_steps = 50000
[2023-11-13 16:49:42,737, DEBUG] : local_rank = 1
[2023-11-13 16:49:42,737, DEBUG] : world_size = 0
[2023-11-13 16:49:44,040, INFO] : Run Name : google/bert_uncased_L-2_H-128_A-2-AdaToken-0.15-91284
[2023-11-13 16:49:44,164, INFO] : Run Name : google/bert_uncased_L-2_H-128_A-2-AdaToken-0.15-91284
