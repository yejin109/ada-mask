[2023-10-26 14:03:22,237, INFO] : Cache at /data1/ay0119/hf-cache
[2023-10-26 14:03:22,237, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231026-1403-AdaToken-0.15-124
[2023-10-26 14:03:22,238, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-10-26 14:03:22,306, INFO] : Cache at /data1/ay0119/hf-cache
[2023-10-26 14:03:22,306, INFO] : Log Dir at /data1/ay0119/bert-tiny-main/all/google/bert_uncased_L-2_H-128_A-2/231026-1403-AdaToken-0.15-124
[2023-10-26 14:03:22,306, INFO] : Model from scratch : google/bert_uncased_L-2_H-128_A-2
[2023-10-26 14:03:29,498, INFO] : bookcorpus raw data 
Dataset({
    features: ['text'],
    num_rows: 74004228
})
[2023-10-26 14:03:29,499, INFO] : wikipedia raw data 
Dataset({
    features: ['text'],
    num_rows: 6458670
})
[2023-10-26 14:03:29,499, INFO] : concat raw data 
Dataset({
    features: ['text'],
    num_rows: 80462898
})
[2023-10-26 14:03:29,499, INFO] : MLM process function started
[2023-10-26 14:03:29,889, INFO] : bookcorpus raw data 
Dataset({
    features: ['text'],
    num_rows: 74004228
})
[2023-10-26 14:03:29,890, INFO] : wikipedia raw data 
Dataset({
    features: ['text'],
    num_rows: 6458670
})
[2023-10-26 14:03:29,890, INFO] : concat raw data 
Dataset({
    features: ['text'],
    num_rows: 80462898
})
[2023-10-26 14:03:29,890, INFO] : MLM process function started
[2023-10-26 14:10:07,429, INFO] : MLM process function Done
[2023-10-26 14:10:07,461, INFO] : Split started
[2023-10-26 14:10:07,609, INFO] : MLM process function Done
[2023-10-26 14:10:07,676, INFO] : Split started
[2023-10-26 14:10:31,063, INFO] : Split Done
[2023-10-26 14:10:31,064, DEBUG] : grouping took 398.11020612716675
[2023-10-26 14:10:31,065, DEBUG] : split took 23.38541293144226
[2023-10-26 14:10:31,335, INFO] : Dataset 
	 DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 43095582
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 431
    })
})
[2023-10-26 14:10:31,336, DEBUG] : seed = 124
[2023-10-26 14:10:31,337, DEBUG] : device = cuda:4
[2023-10-26 14:10:31,338, DEBUG] : cache_dir = /data1/ay0119/hf-cache
[2023-10-26 14:10:31,340, DEBUG] : model_type = google/bert_uncased_L-2_H-128_A-2
[2023-10-26 14:10:31,341, DEBUG] : ckpt = None
[2023-10-26 14:10:31,342, DEBUG] : data = all
[2023-10-26 14:10:31,343, DEBUG] : train_test_split = 1e-05
[2023-10-26 14:10:31,344, DEBUG] : max_seq_length = 128
[2023-10-26 14:10:31,346, DEBUG] : lr = 0.0001
[2023-10-26 14:10:31,347, DEBUG] : lr_scheduler = linear
[2023-10-26 14:10:31,347, DEBUG] : warmup_steps = 10000
[2023-10-26 14:10:31,347, DEBUG] : adam_beta1 = 0.9
[2023-10-26 14:10:31,347, DEBUG] : adam_beta2 = 0.999
[2023-10-26 14:10:31,347, DEBUG] : adam_eps = 1e-08
[2023-10-26 14:10:31,347, DEBUG] : wd = 0.01
[2023-10-26 14:10:31,347, DEBUG] : dropout = 0.1
[2023-10-26 14:10:31,347, DEBUG] : epochs = 40
[2023-10-26 14:10:31,347, DEBUG] : b_train = 128
[2023-10-26 14:10:31,347, DEBUG] : max_steps = 200000
[2023-10-26 14:10:31,347, DEBUG] : gradient_accumulation_steps = 4
[2023-10-26 14:10:31,347, DEBUG] : p = 0.15
[2023-10-26 14:10:31,347, DEBUG] : ada_token = True
[2023-10-26 14:10:31,348, DEBUG] : ada_memo = False
[2023-10-26 14:10:31,348, DEBUG] : cosine = False
[2023-10-26 14:10:31,348, DEBUG] : step = False
[2023-10-26 14:10:31,348, DEBUG] : train = True
[2023-10-26 14:10:31,348, DEBUG] : test = False
[2023-10-26 14:10:31,348, DEBUG] : logging_steps = 20000
[2023-10-26 14:10:31,348, DEBUG] : save_steps = 40000
[2023-10-26 14:10:31,348, DEBUG] : local_rank = 0
[2023-10-26 14:10:31,348, DEBUG] : world_size = 0
[2023-10-26 14:10:32,036, INFO] : Split Done
[2023-10-26 14:10:32,037, DEBUG] : grouping took 397.5389859676361
[2023-10-26 14:10:32,039, DEBUG] : split took 24.57456612586975
[2023-10-26 14:10:32,307, INFO] : Dataset 
	 DatasetDict({
    train: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 43095582
    })
    test: Dataset({
        features: ['input_ids', 'attention_mask'],
        num_rows: 431
    })
})
[2023-10-26 14:10:32,308, DEBUG] : seed = 124
[2023-10-26 14:10:32,309, DEBUG] : device = cuda:4
[2023-10-26 14:10:32,311, DEBUG] : cache_dir = /data1/ay0119/hf-cache
[2023-10-26 14:10:32,312, DEBUG] : model_type = google/bert_uncased_L-2_H-128_A-2
[2023-10-26 14:10:32,313, DEBUG] : ckpt = None
[2023-10-26 14:10:32,314, DEBUG] : data = all
[2023-10-26 14:10:32,315, DEBUG] : train_test_split = 1e-05
[2023-10-26 14:10:32,316, DEBUG] : max_seq_length = 128
[2023-10-26 14:10:32,317, DEBUG] : lr = 0.0001
[2023-10-26 14:10:32,319, DEBUG] : lr_scheduler = linear
[2023-10-26 14:10:32,319, DEBUG] : warmup_steps = 10000
[2023-10-26 14:10:32,319, DEBUG] : adam_beta1 = 0.9
[2023-10-26 14:10:32,319, DEBUG] : adam_beta2 = 0.999
[2023-10-26 14:10:32,319, DEBUG] : adam_eps = 1e-08
[2023-10-26 14:10:32,319, DEBUG] : wd = 0.01
[2023-10-26 14:10:32,319, DEBUG] : dropout = 0.1
[2023-10-26 14:10:32,319, DEBUG] : epochs = 40
[2023-10-26 14:10:32,319, DEBUG] : b_train = 128
[2023-10-26 14:10:32,319, DEBUG] : max_steps = 200000
[2023-10-26 14:10:32,319, DEBUG] : gradient_accumulation_steps = 4
[2023-10-26 14:10:32,319, DEBUG] : p = 0.15
[2023-10-26 14:10:32,319, DEBUG] : ada_token = True
[2023-10-26 14:10:32,319, DEBUG] : ada_memo = False
[2023-10-26 14:10:32,319, DEBUG] : cosine = False
[2023-10-26 14:10:32,319, DEBUG] : step = False
[2023-10-26 14:10:32,319, DEBUG] : train = True
[2023-10-26 14:10:32,319, DEBUG] : test = False
[2023-10-26 14:10:32,319, DEBUG] : logging_steps = 20000
[2023-10-26 14:10:32,319, DEBUG] : save_steps = 40000
[2023-10-26 14:10:32,319, DEBUG] : local_rank = 1
[2023-10-26 14:10:32,319, DEBUG] : world_size = 0
[2023-10-26 14:10:32,891, INFO] : Run Name : google/bert_uncased_L-2_H-128_A-2-AdaToken-0.15-124
[2023-10-26 14:10:33,860, INFO] : Run Name : google/bert_uncased_L-2_H-128_A-2-AdaToken-0.15-124
